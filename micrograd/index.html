<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Neural Network implementation on Google Sheets - Learn how to build and train a neural network using only spreadsheet formulas. Based on Andrej Karpathy's micrograd tutorial with hands-on implementation.">
    <title>Neural Net on Google Sheets - Machine Learning in Spreadsheets</title>
    <link rel="canonical" href="https://www.samuelfu.com/micrograd/">
        <style type="text/css">
        body{margin:40px
            auto;max-width:900px;line-height:1.6;font-size:18px;color:#444;padding:0
            10px}h1,h2,h3{line-height:1.2}        .image-container {
            display: flex;
            flex-direction: row;
            height: 30vh;
            gap: 4vh;
            justify-content: center;
        }
        code {
        font-size:12px;
        background-color: #333; /* Dark grey background */
        color: #fff; /* White text */
        padding: 10px;
        display: block;
        overflow-x: auto; /* Adds horizontal scrolling if necessary */
        }
            </style>
        
</head>
<body>
    <h1>Neural Net on Google Sheets</h1>
    <p>Recently I watched Karpathy's <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">The spelled-out intro to neural networks and backpropagation: building micrograd</a>
        and gained a deeper understanding of neural networks and back propagation. The logic is surprisingly concise, as seen from Karpathy's 200-line implementation of an autograd engine.
        As such, I went ahead and built a neural network on google sheets to test my understanding.
    </p>
    <div class="image-container">
        <img src="/micrograd/micrograd1.png" alt="Image 1">
    </div>
    <p>The network consists of 2 inputs, 2 neurons, and an output.</p>
    <div class="image-container">
        <img src="/micrograd/micrograd2.png" alt="Image 1">
    </div>
    <p>I calculated the forward pass first to obtain the output, and calculate the loss by comparing against the expected output.
        Then with the loss, I am able to calculate the gradient for each step with respect to the loss. Once I have the gradient for
        all the weights and biases, I subtract the learning rate multiplied by the gradient to minimize the loss. After several iterations,
        I am eventually able to lower the loss. Using pytorch and the same number of iterations, I was also able to get a similar loss,
        so I assume with enough iterations eventually the loss can be lowered to near 0.
    </p>
</body>
</html>
